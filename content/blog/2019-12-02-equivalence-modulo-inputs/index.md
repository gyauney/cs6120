+++
title = "Compiler Validation via Equivalence Modulo Inputs"
[extra]
bio = """
  [Rolph Recto](rolph-recto.github.io) is a third-year graduate student studying
  the intersection of programming languages, security, and distributed systems.
  [Gregory Yauney](https://www.cs.cornell.edu/~gyauney)
  is a second-year student working on machine learning and
  digital humanities.
"""
latex = true
[[extra.authors]]
name = "Rolph Recto"
link = "rolph-recto.github.io"
[[extra.authors]]
name = "Gregory Yauney"
link = "https://www.cs.cornell.edu/~gyauney"
+++


Imagine you, being a clever person who wants her C programs to run faster,
sat down, thought very hard, and developed a new compiler optimization.
Say you implement it as a transformation pass in LLVM so that other people
can take advantage of your cleverness to make their programs run faster as well.
You run your optimization pass over a few benchmarks and see that it does
indeed make some programs run faster.
But a question nags you: how do you know that your optimization is correct?
That is, how do you know that your optimization doesn't change the semantics
of the input program?

*Equivalence Modulo Inputs*, a testing technique introduced by Le et al
in a [PLDI 2014 paper][paper], allows our compiler hacker above to test
her optimization rigorously without much effort.

* allows debugging optimizations

[paper]: https://dl.acm.org/citation.cfm?id=2594334


## Some definitions


> **EMI(I)-validity**. Given an input set *I*, a compiler *C* is *EMI(I)-valid*
  if for any program *P* and EMI(I)-variant *Q*, it is the case that
  `C(P)(i) = C(Q)(i)` for all *i* in input set I. 
  **If a compiler is not EMI(I)-valid, then we consider it buggy.** 

But the inverse is not true: if a compiler *is* EMI-valid, it can
still be buggy!
Consider the degenerate compiler that maps all source programs to the same
target program.
The compiler is EMI(I)-valid for any input set *I*, but it is obviously buggy.

Thus EMI-validity is a conservative overapproximation for compiler correctness,
which is still useful for finding bugs in practice.
Its failure to find bugs cannot verify a compiler implementation
(read: absence of evidence is not evidence of absence),
but it can give higher assurance that it works as intended.

## Background

EMI is a form of [differential testing][], the widely applicable idea that if
multiple systems presumed to be equivalent produce different outputs on the same
input, then there is a bug in at least one of the systems.

[Csmith][] also differentially tests compilers by generating random test cases,
and it led to 281 bug reports for GCC and LLVM by the time it was published.
Whereas EMI compares output from different programs compiled by the same
compiler, Csmith compares output from multiple compilers on the same input
program.

Explicitly wanting to avoid Csmith's painstaking approach to restricting random
program generation with aggressive safety analysis, Le & al. design their
implementation of EMI around generating equivalent variants of valid seed
programs, as we will see below. The EMI and Csmith approaches are not
oppositional; in fact, Le & al. incorporate Csmith into their workflow. The vast
majority (all but four) of bugs identified by EMI were found with EMI variants
of random programs generated by Csmith.

[Differential testing]: https://www.cs.swarthmore.edu/~bylvisa1/cs97/f13/Papers/DifferentialTestingForSoftware.pdf
[Csmith]: https://dl.acm.org/citation.cfm?id=1993532

## EMI in Practice: Orion

How do we


## Evaluation

In order to evaluate EMI&mdash;even in its concrete implementation
Orion&mdash;several questions must be answered:

1. _What compilers (and compiler configurations) will be tested?_\
\
The authors test GCC and LLVM, popular open-source compilers with transparent
bug tracking. The latest development builds of the compilers were tested on an
x86_64 machine, targeting both 32- and 64-bit machines. Because the goal is to
find miscompilations arising from optimizations, the common optimization
configurations were all tested: `-O0`, `-O1`, `-Os`, `-O2`, `-O3`.

2. _What seed programs will be profiled and pruned?_\
\
Some seed programs were taken from the GCC, LLVM, and KCC regression test
suites. The authors report attempting to use tests from open-source projects,
but were unable to reduce and interpret the resulting bugs.\
\
The bulk of the bugs were found by starting from randomly generated Csmith
programs, likely because each consisted of, on average, thousands of lines of
code with a high proportion of unexecuted lines.\
\
Though the compiler test programs were verified to be correct by experts,
there was no one verifying that the random Csmith programs produced correct
output. Only equivalence (preserved by the pruning process) is necessary to
ensure EMI variants are able to detect bugs, greatly increasing the pool of seed
programs.

3. _What parameters will guide the pruning process?_\
\
Each seed program had a random number of variants generated, with an expectation
of eight variants. The two random parameters that control the likelihood of a
given statement getting pruned were independently reset to a uniform new value
between 0 and 1 after each pruning.

4. _What will be done with bugs once any have been found?_\
\
The authors used a combination of C-reduce and Berkeley Delta to shrink the size
of EMI programs that generated different outputs. They attempted to reject
programs that triggered undefined behavior by using compiler warnings, static
analysis, and CompCert. The final step was reporting the bugs using the
compilers' transparent bug-tracking tools.

With this context, on to the headline result:

**Orion found 147 confirmed, unique bugs in GCC and LLVM over the course of
eleven months in 2013.**

Le & al. evaluate their bugs in a twofold way: 1) quantitative description of
components affected by bugs, and 2) qualitative evaluation of about ten
generated programs.

#### Quantitative description

A major strength of the evaluation is its integration with the bug reporting
workflows for GCC and LLVM. While the authors go perhaps too far in asserting,
"First, most of our reported bugs have been confirmed and fixed by developers,
illustrating their relevance and importance (as it often takes substantial
effort to fix a miscompilation)," the fact that 182 of the 195 total reported
bugs (with 35 of these getting marked duplicate) were confirmed by outside
experts to really exist is evidence that EMI is a viable bug-finding strategy.

95 of the confirmed bugs were miscompilations, lending credence to the authors'
initial claim that Orion is able to target miscompilations more easily than
Csmith alone can. The most bugs were found in the trunks of both GCC and LLVM.
More bugs were also found in increasing levels of optimization, with the most
under `-O3`.

The authors also found performance bugs through comparing compilers to each
other, in a differential testing scenario similar to that used by Csmith. 19 of
the 147 confirmed bugs were performance issues.

It's important to note that these were only the bugs that were found by Orion.
Because Orion specifically targeted optimization phases, it is understandable
that GCC Tree Optimization and RTL optimization were the components with the
most discovered bugs (LLVM developers did not classify the reported bugs). These
components did not necessarily have more bugs than others, nor were these the
only possible bugs.

The authors do not make an attempt to evaluate the search space that Orion
explored in producing these reported bugs. Nor do they explicitly determine the
proportion of the generated variants that led to identified bugs. They only
report that they didn't record how many seed programs they started with or how
many variants they generated (merely estimating "millions to tens of millions").
They also do not report (and likely did not record) the Csmith configurations or
Orion's dynamic pruning parameters.

#### Qualitative examples

The confirmed bugs are said to span compiler segfaults, internal compiler
errors, performance problems, and wrong code generation. The authors present and
interpret a handful of the bugs that were confirmed and fixed by compiler
developers. We highlight just two of those for a flavor of the generated
programs. Note that the authors only show the reduced code they reported to
compiler developers; they show neither the non-reduced versions nor the EMI
variants.

The following example led to a segfault when compiled with GCC due to a wrong
offset computation in an optimization pass called "predictive commoning," a form
of common subexpression elimination:

```c
int b, f, d[5][2];
unsigned int c;
int main() {
  for (c = 0; c < 2; c++)
    if (d[b + 3][c] & d[b + 4][c])
      if (f)
        break;
  return 0;
}
```

Clang incorrectly vectorized the following code:

```c
int main() {
  int a = 1;
  char b = 0;
  lbl:
    a &= 4;
    b--;
    if (b) goto lbl;
  return a;
}
```

The remaining examples in the paper cover problems with jump-threading logic,
global value numbering, inlining, vectorization, and performance. Because the
authors analyze only a few cherry-picked examples, a question remains: Are these
eight examples representative of all of the other bugs?

Additionally, the authors claim: "EMI variants generated from existing code, say
via Orion, are likely programs that people actually write."
Is this true, especially when random programs are used as seeds?
Is this even true of the two examples discussed above?



